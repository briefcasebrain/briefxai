Metadata-Version: 2.1
Name: briefx
Version: 2.0.0
Summary: High-performance conversation analysis platform
Home-page: UNKNOWN
Author: BriefX Team
License: UNKNOWN
Platform: UNKNOWN
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Requires-Python: >=3.8
Description-Content-Type: text/markdown

# BriefX

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Build Status](https://img.shields.io/badge/build-passing-brightgreen.svg)](https://github.com/briefcasebrain/briefxai)

A high-performance conversation analysis platform for extracting insights from conversational data at scale.

## Overview

BriefX provides enterprise-grade conversation analysis capabilities with advanced clustering algorithms and pattern discovery methodologies. Built for performance, privacy, and scalability.

## Features

### Core Capabilities
- **Advanced Clustering** - Multi-level conversation grouping with hierarchical analysis
- **Facet Extraction** - Automatic identification of topics, sentiments, intents, and entities  
- **Privacy Protection** - PII detection and threshold-based anonymization
- **Real-time Processing** - Stream processing with WebSocket support
- **Multi-provider Support** - Compatible with various LLM providers (OpenAI, Anthropic, local models)

### Technical Features  
- Concurrent processing for high throughput
- Session persistence with pause/resume capability
- Efficient caching and batch processing
- REST and WebSocket APIs
- UMAP dimensionality reduction for visualization
- Smart preprocessing with language detection

## Quick Start

### Python Implementation

1. **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

2. **Set Environment Variables**
   ```bash
   export OPENAI_API_KEY="your-api-key-here"
   # or
   export ANTHROPIC_API_KEY="your-api-key-here" 
   ```

3. **Run Web Interface**
   ```bash
   python app.py
   ```

4. **Access Interface**
   Open http://localhost:8080 in your browser

### Rust Implementation

1. **Build Application**
   ```bash
   cargo build --release
   ```

2. **Run Server**
   ```bash
   ./target/release/briefx ui --port 8080
   ```

## Usage Examples

### Python API

```python
from src.data.models import ConversationData, Message
from src.analysis.pipeline import AnalysisPipeline

# Create conversation data
conversations = [
    ConversationData(messages=[
        Message(role="user", content="I need help with my order"),
        Message(role="assistant", content="I'd be happy to help with your order")
    ])
]

# Analyze conversations
pipeline = AnalysisPipeline()
results = pipeline.analyze(conversations)
```

### REST API

```bash
# Upload conversations
curl -X POST http://localhost:8080/api/conversations \
  -H "Content-Type: application/json" \
  -d '{"conversations": [{"messages": [{"role": "user", "content": "Hello"}]}]}'

# Get analysis results
curl http://localhost:8080/api/analysis/results
```

## Configuration

### Environment Variables
- `OPENAI_API_KEY` - OpenAI API key
- `ANTHROPIC_API_KEY` - Anthropic API key
- `BRIEFX_PORT` - Server port (default: 8080)
- `BRIEFX_HOST` - Server host (default: 127.0.0.1)
- `BRIEFX_LOG_LEVEL` - Log level (default: INFO)

### Configuration File
Create `config.toml`:

```toml
[server]
host = "127.0.0.1"
port = 8080

[analysis]
batch_size = 100
max_concurrent_requests = 10

[providers.openai]
enabled = true
model = "gpt-4"
api_key = "${OPENAI_API_KEY}"
```

See `docs/configuration.md` for complete configuration options.

## Architecture

- **Data Models** - Structured conversation and analysis data types
- **Preprocessing** - Text normalization, validation, and language detection
- **Analysis Pipeline** - Clustering, facet extraction, and pattern discovery
- **Provider System** - Pluggable LLM and embedding provider architecture
- **Web Interface** - Interactive visualization and analysis tools
- **Persistence** - Session management and result caching

## Development

### Python Development

```bash
# Install development dependencies
pip install -r requirements.txt

# Run tests
python -m pytest test_complete.py

# Run linting
flake8 src/

# Start development server
python app.py --debug
```

### Rust Development

```bash
# Run tests
cargo test

# Run with debug logging
RUST_LOG=debug cargo run -- ui

# Format code
cargo fmt

# Check for issues
cargo clippy
```

## API Documentation

### Core Endpoints

- `GET /` - Web interface
- `POST /api/conversations` - Upload conversation data
- `GET /api/analysis/results` - Get analysis results
- `POST /api/analysis/start` - Start analysis session
- `GET /api/monitoring/health` - Health check

### WebSocket Endpoints

- `/ws/analysis` - Real-time analysis updates
- `/ws/progress` - Analysis progress updates

See `docs/api.md` for complete API documentation.

## Performance

### Benchmarks (Python)
- **Throughput**: 1000+ conversations/minute
- **Memory Usage**: ~512MB for 10k conversations  
- **Clustering**: 100 conversations clustered in <5 seconds
- **Facet Extraction**: 50ms average per conversation

### Benchmarks (Rust)
- **Throughput**: 5000+ conversations/minute
- **Memory Usage**: ~128MB for 10k conversations
- **Clustering**: 1000 conversations clustered in <2 seconds
- **Facet Extraction**: 10ms average per conversation

## Privacy & Security

- **PII Detection** - Automatic detection of emails, phone numbers, addresses
- **Data Anonymization** - Configurable masking and removal policies
- **Local Processing** - Option to run entirely offline with local models
- **Secure Storage** - Encrypted data persistence options
- **Access Control** - API key authentication and rate limiting

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Contributing

Please read [CONTRIBUTING.md](CONTRIBUTING.md) for details on our code of conduct and the process for submitting pull requests.

## Support

For questions and support:
- Check the [documentation](docs/)
- Review [examples](src/examples.py)
- Open an issue on GitHub

## Changelog

See [CHANGELOG.md](CHANGELOG.md) for version history and updates.

