# Configuration Guide

BriefXAI offers extensive configuration options through TOML files, environment variables, and command-line arguments.

## Configuration Priority

Configuration sources are applied in this order (highest to lowest priority):

1. Command-line arguments
2. Environment variables
3. User configuration file (`~/.config/briefxai/config.toml`)
4. Project configuration file (`./config.toml`)
5. System configuration file (`/etc/briefxai/config.toml`)
6. Default values

## Configuration File Format

BriefXAI uses TOML format for configuration files. Here's a complete example:

```toml
# config.toml - Complete configuration example

# Server Configuration
[server]
host = "127.0.0.1"              # Bind address
port = 8080                     # Port number
workers = 4                     # Number of worker threads
max_connections = 1000          # Maximum concurrent connections
request_timeout_ms = 30000      # Request timeout in milliseconds
cors_enabled = true             # Enable CORS
cors_origins = ["*"]            # Allowed CORS origins

# Database Configuration
[database]
path = "data/briefxai.db"       # SQLite database path
max_connections = 25            # Connection pool size
busy_timeout_ms = 5000          # Busy timeout for SQLite
journal_mode = "WAL"            # SQLite journal mode
synchronous = "NORMAL"          # SQLite synchronous mode
cache_size_kb = 2000           # SQLite cache size in KB

# Analysis Configuration
[analysis]
batch_size = 100                # Conversations per batch
max_concurrent_requests = 10    # Concurrent LLM requests
timeout_seconds = 300           # Analysis timeout
max_retries = 3                 # Retry attempts for failures
retry_delay_ms = 1000          # Delay between retries
cache_enabled = true            # Enable result caching
cache_ttl_seconds = 3600       # Cache time-to-live

# Preprocessing Configuration
[preprocessing]
detect_duplicates = true        # Enable duplicate detection
duplicate_threshold = 0.95      # Similarity threshold for duplicates
detect_pii = true              # Enable PII detection
pii_action = "mask"            # PII action: "mask", "remove", or "flag"
validate_format = true          # Validate conversation format
min_message_length = 1          # Minimum message length
max_message_length = 10000      # Maximum message length
detect_language = true          # Enable language detection
supported_languages = ["en", "es", "fr", "de", "zh", "ja"]

# Facet Extraction Configuration
[facets]
enabled = true                  # Enable facet extraction
default_facets = [              # Default facets to extract
    "sentiment",
    "topics",
    "entities",
    "intents",
    "issues",
    "resolutions"
]
custom_prompts_path = "prompts/" # Path to custom prompt templates
max_facets_per_conversation = 50 # Maximum facets per conversation

# Embedding Configuration
[embeddings]
model = "text-embedding-ada-002" # Embedding model
dimensions = 1536                # Embedding dimensions
batch_size = 100                 # Embedding batch size
cache_enabled = true             # Cache embeddings
normalize = true                 # Normalize embeddings

# Clustering Configuration
[clustering]
algorithm = "kmeans"             # Clustering algorithm: "kmeans" or "hierarchical"
min_clusters = 2                 # Minimum number of clusters
max_clusters = 20                # Maximum number of clusters
auto_detect = true               # Auto-detect optimal clusters
distance_metric = "cosine"       # Distance metric
max_iterations = 100             # Maximum iterations for k-means
convergence_threshold = 0.0001   # Convergence threshold

# Provider Configuration - OpenAI
[providers.openai]
enabled = true                   # Enable this provider
api_key = "${OPENAI_API_KEY}"   # API key (can use env vars)
base_url = "https://api.openai.com/v1"
model = "gpt-4"                 # Default model
temperature = 0.7                # Generation temperature
max_tokens = 2000               # Maximum tokens per request
timeout_ms = 30000              # Request timeout
max_retries = 3                 # Retry attempts
rate_limit_rpm = 60             # Requests per minute limit

# Provider Configuration - Ollama (Local)
[providers.ollama]
enabled = false                  # Enable this provider
base_url = "http://localhost:11434"
model = "llama2"                # Default model
temperature = 0.7
max_tokens = 2000
timeout_ms = 60000              # Longer timeout for local models
max_concurrent = 4              # Concurrent requests

# Provider Configuration - Custom
[providers.custom]
enabled = false
base_url = "https://your-api.com"
api_key = "${CUSTOM_API_KEY}"
model = "your-model"
headers = { "X-Custom-Header" = "value" }

# Load Balancing Configuration
[load_balancing]
strategy = "round_robin"         # Strategy: "round_robin", "least_latency", "weighted", "cost_optimized"
health_check_interval_seconds = 30
failover_enabled = true          # Enable automatic failover
failover_threshold = 3           # Failures before failover

# Circuit Breaker Configuration
[circuit_breaker]
enabled = true                   # Enable circuit breaker
failure_threshold = 5            # Failures to open circuit
success_threshold = 2            # Successes to close circuit
timeout_seconds = 60             # Time before half-open
half_open_requests = 3           # Requests in half-open state

# Monitoring Configuration
[monitoring]
enabled = true                   # Enable monitoring
metrics_port = 9090             # Prometheus metrics port
collect_interval_seconds = 10   # Metrics collection interval
export_format = "prometheus"    # Export format

# Logging Configuration
[logging]
level = "info"                   # Log level: "trace", "debug", "info", "warn", "error"
format = "json"                  # Log format: "json" or "text"
output = "stdout"                # Output: "stdout", "stderr", or file path
file_rotation = "daily"          # Rotation: "daily", "size", or "never"
max_file_size_mb = 100          # Maximum log file size
max_files = 7                    # Maximum number of log files
include_timestamp = true         # Include timestamps
include_location = false         # Include code location

# Security Configuration
[security]
api_key_required = false         # Require API key authentication
api_keys_file = "api_keys.json" # Path to API keys file
rate_limiting_enabled = true     # Enable rate limiting
rate_limit_rpm = 100            # Requests per minute per client
max_request_size_mb = 10        # Maximum request size
allowed_origins = ["*"]          # Allowed origins for CORS
enable_tls = false              # Enable TLS/SSL
tls_cert_path = ""              # TLS certificate path
tls_key_path = ""               # TLS key path

# Export Configuration
[export]
formats = ["json", "csv", "html"] # Supported export formats
max_export_size_mb = 100         # Maximum export size
compression_enabled = true        # Enable compression
include_metadata = true          # Include metadata in exports

# Template Configuration
[templates]
path = "templates/"              # Template directory
builtin = [                     # Built-in templates
    "customer_support",
    "sales",
    "medical",
    "technical_support"
]
custom_enabled = true           # Enable custom templates
validation_strict = true        # Strict template validation

# Cache Configuration
[cache]
type = "memory"                 # Cache type: "memory" or "disk"
max_size_mb = 500              # Maximum cache size
ttl_seconds = 3600             # Default TTL
eviction_policy = "lru"        # Eviction policy: "lru", "lfu", "fifo"
```

## Environment Variables

All configuration values can be overridden using environment variables. Use the following format:

```bash
BRIEFXAI_<SECTION>_<KEY>=value
```

Examples:

```bash
# Server configuration
export BRIEFXAI_SERVER_HOST=0.0.0.0
export BRIEFXAI_SERVER_PORT=8080

# Database configuration
export BRIEFXAI_DATABASE_PATH=/var/lib/briefxai/data.db

# Provider configuration
export BRIEFXAI_PROVIDERS_OPENAI_API_KEY=sk-your-api-key
export BRIEFXAI_PROVIDERS_OPENAI_MODEL=gpt-4

# Logging configuration
export BRIEFXAI_LOGGING_LEVEL=debug
```

## Command-Line Arguments

Override configuration at runtime:

```bash
briefxai serve \
  --host 0.0.0.0 \
  --port 8080 \
  --config custom-config.toml \
  --log-level debug
```

Available arguments:

```bash
briefxai serve --help

OPTIONS:
    --host <HOST>           Server host address
    --port <PORT>           Server port
    --config <PATH>         Configuration file path
    --database <PATH>       Database file path
    --log-level <LEVEL>     Log level (trace|debug|info|warn|error)
    --workers <NUM>         Number of worker threads
    --api-key <KEY>         API key for authentication
```

## Configuration Templates

### Minimal Configuration

```toml
# Minimal config for quick start
[server]
port = 8080

[providers.openai]
api_key = "your-api-key"
```

### Development Configuration

```toml
# Development configuration with debugging
[server]
host = "127.0.0.1"
port = 8080

[database]
path = "dev.db"

[logging]
level = "debug"
format = "text"

[providers.ollama]
enabled = true
base_url = "http://localhost:11434"
```

### Production Configuration

```toml
# Production configuration with security and monitoring
[server]
host = "0.0.0.0"
port = 443
workers = 16

[database]
path = "/var/lib/briefxai/prod.db"
max_connections = 50

[security]
api_key_required = true
enable_tls = true
tls_cert_path = "/etc/ssl/certs/briefxai.crt"
tls_key_path = "/etc/ssl/private/briefxai.key"

[monitoring]
enabled = true
metrics_port = 9090

[logging]
level = "warn"
format = "json"
output = "/var/log/briefxai/app.log"
```

## Provider-Specific Configuration

### OpenAI

```toml
[providers.openai]
api_key = "${OPENAI_API_KEY}"
organization_id = "org-xxxxx"  # Optional
model = "gpt-4"
temperature = 0.7
top_p = 1.0
frequency_penalty = 0.0
presence_penalty = 0.0
```

### Anthropic Claude

```toml
[providers.anthropic]
api_key = "${ANTHROPIC_API_KEY}"
model = "claude-3-opus-20240229"
max_tokens = 4096
temperature = 0.7
```

### Local Models (Ollama)

```toml
[providers.ollama]
base_url = "http://localhost:11434"
model = "mixtral"
num_ctx = 4096          # Context window size
num_gpu = 1             # Number of GPUs to use
num_thread = 8          # Number of CPU threads
```

### Azure OpenAI

```toml
[providers.azure]
api_key = "${AZURE_API_KEY}"
endpoint = "https://your-resource.openai.azure.com"
deployment_name = "your-deployment"
api_version = "2023-12-01-preview"
```

## Advanced Configuration

### Multi-Provider Setup

```toml
# Primary provider
[providers.openai]
enabled = true
api_key = "${OPENAI_API_KEY}"
model = "gpt-4"
priority = 1

# Fallback provider
[providers.ollama]
enabled = true
base_url = "http://localhost:11434"
model = "llama2"
priority = 2

# Load balancing
[load_balancing]
strategy = "weighted"
weights = { openai = 70, ollama = 30 }
```

### Custom Prompt Templates

```toml
[facets]
custom_prompts_path = "prompts/"

[facets.custom_prompts.customer_satisfaction]
prompt = """
Analyze the customer satisfaction level:
- Very Satisfied
- Satisfied
- Neutral
- Dissatisfied
- Very Dissatisfied
"""
```

### Performance Tuning

```toml
[performance]
# Connection pooling
connection_pool_size = 50
connection_timeout_ms = 5000

# Request handling
max_concurrent_requests = 100
request_queue_size = 1000
request_timeout_ms = 30000

# Memory management
max_memory_mb = 4096
gc_interval_seconds = 60

# Threading
worker_threads = 16
blocking_threads = 512
```

## Validation

Validate your configuration:

```bash
briefxai config validate --file config.toml
```

Check effective configuration:

```bash
briefxai config show --format json
```

## Troubleshooting

### Common Issues

1. **Provider API Key Issues**
   ```toml
   # Use environment variable
   api_key = "${OPENAI_API_KEY}"
   # Or direct value (not recommended for production)
   api_key = "sk-xxxxx"
   ```

2. **Database Permission Issues**
   ```toml
   [database]
   path = "/tmp/briefxai.db"  # Use writable directory
   ```

3. **Port Already in Use**
   ```toml
   [server]
   port = 8081  # Use alternative port
   ```

### Debug Configuration

Enable debug logging to troubleshoot:

```toml
[logging]
level = "trace"
include_location = true

[debug]
print_config_on_start = true
validate_config = true
```

## Best Practices

1. **Use Environment Variables for Secrets**
   - Never commit API keys to version control
   - Use `${VAR_NAME}` syntax in config files

2. **Separate Development and Production Configs**
   - `config.dev.toml` for development
   - `config.prod.toml` for production

3. **Monitor Resource Usage**
   - Set appropriate limits for your hardware
   - Monitor metrics to optimize settings

4. **Regular Backups**
   - Configure database backups
   - Export configurations for disaster recovery

5. **Security Hardening**
   - Enable API key authentication in production
   - Use TLS for encrypted connections
   - Implement rate limiting

## Migration from Previous Versions

### From v1.x to v2.x

1. Configuration file format changed from JSON to TOML
2. New required fields:
   - `providers` section is now mandatory
   - `analysis.batch_size` replaces `batch_size`

3. Deprecated fields:
   - `llm_provider` → use `providers` section
   - `embedding_model` → use `embeddings.model`

Migration script:

```bash
briefxai config migrate --from v1 --input old-config.json --output config.toml
```